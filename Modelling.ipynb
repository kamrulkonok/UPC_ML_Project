{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11698fe",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "After cleaning and preprocessing the data, we model the data distributions by trying to fit it into some well known models. We execute the following steps to do a fair modelling : \n",
    "\n",
    "1. Splitting the Data into Training and Test Sets\n",
    "\n",
    "2. Hyperparameter Tuning : Models, hyperparameters and loss function\n",
    "\n",
    "3. Evaluation of Models on the Test Data\n",
    "\n",
    "4. Retraining the Best Model on the complete training set for prediction.\n",
    "\n",
    "5. Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efbd4c7",
   "metadata": {},
   "source": [
    "# Import Libraries & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c95fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e97e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"preprocessed_airbnb_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a493ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>multi</th>\n",
       "      <th>biz</th>\n",
       "      <th>cleanliness_rating</th>\n",
       "      <th>guest_satisfaction_overall</th>\n",
       "      <th>attr_index_norm</th>\n",
       "      <th>rest_index_norm</th>\n",
       "      <th>room_type_Entire home/apt</th>\n",
       "      <th>room_type_Private room</th>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <th>city_amsterdam</th>\n",
       "      <th>city_athens</th>\n",
       "      <th>city_barcelona</th>\n",
       "      <th>city_berlin</th>\n",
       "      <th>city_budapest</th>\n",
       "      <th>city_lisbon</th>\n",
       "      <th>city_london</th>\n",
       "      <th>city_paris</th>\n",
       "      <th>city_rome</th>\n",
       "      <th>city_vienna</th>\n",
       "      <th>day_type_weekdays</th>\n",
       "      <th>day_type_weekends</th>\n",
       "      <th>space_index</th>\n",
       "      <th>metro_to_city_dist_ratio</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.575995</td>\n",
       "      <td>-0.661727</td>\n",
       "      <td>-0.441734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.563830</td>\n",
       "      <td>1.043721</td>\n",
       "      <td>-0.253920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.290887</td>\n",
       "      <td>1.989541</td>\n",
       "      <td>1.685065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.032003</td>\n",
       "      <td>1.011956</td>\n",
       "      <td>0.707075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.347063</td>\n",
       "      <td>-0.678112</td>\n",
       "      <td>-0.449985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.563830</td>\n",
       "      <td>1.278779</td>\n",
       "      <td>0.262849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.448981</td>\n",
       "      <td>1.327768</td>\n",
       "      <td>1.793710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.031651</td>\n",
       "      <td>1.883336</td>\n",
       "      <td>1.093609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.857603</td>\n",
       "      <td>1.613574</td>\n",
       "      <td>1.621825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.563830</td>\n",
       "      <td>1.194023</td>\n",
       "      <td>1.283568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_is_superhost  multi  biz  cleanliness_rating  \\\n",
       "0                  0      1    0               1.000   \n",
       "1                  0      0    0               0.750   \n",
       "2                  0      0    1               0.875   \n",
       "3                  0      0    1               0.875   \n",
       "4                  1      0    0               1.000   \n",
       "\n",
       "   guest_satisfaction_overall  attr_index_norm  rest_index_norm  \\\n",
       "0                    0.575995        -0.661727        -0.441734   \n",
       "1                    0.290887         1.989541         1.685065   \n",
       "2                    0.347063        -0.678112        -0.449985   \n",
       "3                    0.448981         1.327768         1.793710   \n",
       "4                    0.857603         1.613574         1.621825   \n",
       "\n",
       "   room_type_Entire home/apt  room_type_Private room  room_type_Shared room  \\\n",
       "0                        0.0                     1.0                    0.0   \n",
       "1                        0.0                     1.0                    0.0   \n",
       "2                        0.0                     1.0                    0.0   \n",
       "3                        0.0                     1.0                    0.0   \n",
       "4                        0.0                     1.0                    0.0   \n",
       "\n",
       "   city_amsterdam  city_athens  city_barcelona  city_berlin  city_budapest  \\\n",
       "0             1.0          0.0             0.0          0.0            0.0   \n",
       "1             1.0          0.0             0.0          0.0            0.0   \n",
       "2             1.0          0.0             0.0          0.0            0.0   \n",
       "3             1.0          0.0             0.0          0.0            0.0   \n",
       "4             1.0          0.0             0.0          0.0            0.0   \n",
       "\n",
       "   city_lisbon  city_london  city_paris  city_rome  city_vienna  \\\n",
       "0          0.0          0.0         0.0        0.0          0.0   \n",
       "1          0.0          0.0         0.0        0.0          0.0   \n",
       "2          0.0          0.0         0.0        0.0          0.0   \n",
       "3          0.0          0.0         0.0        0.0          0.0   \n",
       "4          0.0          0.0         0.0        0.0          0.0   \n",
       "\n",
       "   day_type_weekdays  day_type_weekends  space_index  \\\n",
       "0                1.0                0.0    -0.563830   \n",
       "1                1.0                0.0    -0.032003   \n",
       "2                1.0                0.0    -0.563830   \n",
       "3                1.0                0.0     1.031651   \n",
       "4                1.0                0.0    -0.563830   \n",
       "\n",
       "   metro_to_city_dist_ratio     price  \n",
       "0                  1.043721 -0.253920  \n",
       "1                  1.011956  0.707075  \n",
       "2                  1.278779  0.262849  \n",
       "3                  1.883336  1.093609  \n",
       "4                  1.194023  1.283568  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to run locally for Aryan\n",
    "# df = pd.read_csv(\"./preprocessed_airbnb_data.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace65a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51707 entries, 0 to 51706\n",
      "Data columns (total 25 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   host_is_superhost           51707 non-null  int64  \n",
      " 1   multi                       51707 non-null  int64  \n",
      " 2   biz                         51707 non-null  int64  \n",
      " 3   cleanliness_rating          51707 non-null  float64\n",
      " 4   guest_satisfaction_overall  51707 non-null  float64\n",
      " 5   attr_index_norm             51707 non-null  float64\n",
      " 6   rest_index_norm             51707 non-null  float64\n",
      " 7   room_type_Entire home/apt   51707 non-null  float64\n",
      " 8   room_type_Private room      51707 non-null  float64\n",
      " 9   room_type_Shared room       51707 non-null  float64\n",
      " 10  city_amsterdam              51707 non-null  float64\n",
      " 11  city_athens                 51707 non-null  float64\n",
      " 12  city_barcelona              51707 non-null  float64\n",
      " 13  city_berlin                 51707 non-null  float64\n",
      " 14  city_budapest               51707 non-null  float64\n",
      " 15  city_lisbon                 51707 non-null  float64\n",
      " 16  city_london                 51707 non-null  float64\n",
      " 17  city_paris                  51707 non-null  float64\n",
      " 18  city_rome                   51707 non-null  float64\n",
      " 19  city_vienna                 51707 non-null  float64\n",
      " 20  day_type_weekdays           51707 non-null  float64\n",
      " 21  day_type_weekends           51707 non-null  float64\n",
      " 22  space_index                 51707 non-null  float64\n",
      " 23  metro_to_city_dist_ratio    51707 non-null  float64\n",
      " 24  price                       51707 non-null  float64\n",
      "dtypes: float64(22), int64(3)\n",
      "memory usage: 9.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80179f",
   "metadata": {
    "id": "cxnV9reFmiMT"
   },
   "source": [
    "# 1. Splitting the Data into Train and Test Sets.\n",
    "\n",
    "It is good practice to split the data into training, validation and test sets. Validation set is useful for hyperparameter tuning and model selection. Therefore, during hyperparameter tuning, we apply K fold cross validation. It is important to keep the test set completely independent to make sure that the model does not overfit and that there is no data leakage between the train and test set.\n",
    "\n",
    "80:20 split for training and test set is an accepted split to use, since it provides enough data for cross validation based hyperparameter tuning and test set, as we have a large amount of data (50,000 + data samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a26962d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23nPL-LZZJYA",
    "outputId": "20ccabac-7d3a-43dd-dac3-31080fc75286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (41365, 24)\n",
      "Testing set shape: (10342, 24)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# splitting the data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fa900",
   "metadata": {},
   "source": [
    "# 2. Creating Pipelines & Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c62c4",
   "metadata": {},
   "source": [
    "## A. Models and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422fab8",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "### 1. Linear Regression\n",
    "\n",
    "- `fit_intercept` :  whether to calculate the intercept for the model.\n",
    "- `normalize` :  X will be normalized before regression.\n",
    "\n",
    "### 2. Ridge Regression \n",
    "\n",
    "- `alpha` : Constant that multiplies the L2 term, controlling regularization strength\n",
    "- `fit_intercept` :  whether to calculate the intercept for the model.\n",
    "\n",
    "### 3. Lasso Regression\n",
    "\n",
    "- `alpha` : Constant that multiplies the L1 term, controlling regularization strength.\n",
    "- `fit_intercept` :  whether to calculate the intercept for the model.\n",
    "\n",
    "### 4. Polynomial Regression\n",
    "\n",
    "- `poly_features__degree` : Choose polynomial degree\n",
    "- `fit_intercept` :  whether to calculate the intercept for the model.\n",
    "\n",
    "## Tree Models (Non - Linear Models)\n",
    "\n",
    "### 5. Random Forest Regression\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "- `n_estimators`: Number of trees in the forest. Common values are around 100-300, but this can be tuned.\n",
    "- `max_depth`: Maximum depth of the trees. None means nodes are expanded until all leaves are pure or until they contain fewer than min_samples_split samples.\n",
    "- `min_samples_split`: Minimum number of samples required to split an internal node. Increasing this value can make the model more robust by reducing overfitting.\n",
    "- `min_samples_leaf`: Minimum number of samples required to be at a leaf node. A similar effect to min_samples_split.\n",
    "- `max_features`: Number of features to consider when looking for the best split. Common values include 'auto' (all features), 'sqrt' (square root of the number of features), and 'log2' (logarithm base 2 of the number of features).\n",
    "\n",
    "### 6. Decision Tree Regression\n",
    "\n",
    "- `max_depth` : maximum depth of the tree\n",
    "- `min_samples_split` : minimum number of samples required to split an internal node\n",
    "- `min_samples_leaf` : minimum number of samples required to be at a leaf node.\n",
    "\n",
    "### 7. Gradient Boosting Regression\n",
    "\n",
    "- `n_estimators` : The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "- `learning_rate` : Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "- `max_depth` : Maximum depth of the individual regression estimators.\n",
    "- `min_samples_split` : minimum number of samples required to split an internal node\n",
    "- `min_samples_leaf` : minimum number of samples required to be at a leaf node\n",
    "\n",
    "### 8. XGBoost\n",
    "\n",
    "- `n_estimators`: Number of trees.\n",
    "- `learning_rate`: Step size shrinkage used to prevent overfitting.\n",
    "- `max_depth`: Maximum depth of a tree.\n",
    "- `min_child_weight`: Minimum sum of instance weight needed in a child.\n",
    "- `subsample`: Subsample ratio of the training instance.\n",
    "- `colsample_bytree`: Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "## Support Vector Machine\n",
    "\n",
    "### 9. Support Vector Regression\n",
    "- `kernel` : Specifies the kernel type to be used in the algorithm (‘linear’, ‘poly’, ‘rbf’ etc)\n",
    "- `C` : Regularization parameter. The strength of the regularization is inversely proportional to C. Penalty is a squared l2.\n",
    "- `gamma` : Kernel coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b47174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines for each model\n",
    "models = {\n",
    "    'linear_regression': LinearRegression(),\n",
    "    'ridge_regression': Ridge(),\n",
    "    'lasso_regression': Lasso(),\n",
    "    'polynomial_regression': Pipeline([\n",
    "        ('poly_features', PolynomialFeatures()),\n",
    "        ('linear_regression', LinearRegression())]),\n",
    "    'random_forest': RandomForestRegressor(random_state=42),\n",
    "    'decision_tree': DecisionTreeRegressor(random_state=42),\n",
    "    'gradient_boosting': GradientBoostingRegressor(random_state=42),\n",
    "      'xgboost': XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "    'support_vector_regression': SVR(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6901510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'linear_regression': {'model__fit_intercept': [True, False]},\n",
    "    'ridge_regression': {'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], 'model__fit_intercept': [True, False]},\n",
    "    'lasso_regression': {'model__alpha' : [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0], 'model__fit_intercept': [True, False]},\n",
    "    'polynomial_regression': {'model__poly_features__degree': [2, 3]},\n",
    "    'random_forest': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'model__min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node\n",
    "        'model__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "    },\n",
    "    'decision_tree' : {'model__max_depth': [None, 10, 20, 30],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2, 4]},\n",
    "    'gradient_boosting': {\n",
    "        'model__n_estimators': [50, 100, 200, 300],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2, 4],\n",
    "        'model__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'model__subsample': [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model__n_estimators': [100, 200, 300],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__min_child_weight': [1, 3, 5],\n",
    "        'model__subsample': [0.6, 0.8, 1.0],\n",
    "        'model__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'model__gamma': [0, 0.1, 0.2, 0.3],\n",
    "        'model__reg_alpha': [0, 0.1, 1],\n",
    "        'model__reg_lambda': [1, 10]\n",
    "    },\n",
    "    'support_vector_regression': {\n",
    "        'model__C': [0.1, 1.0, 10.0], \n",
    "        'model__kernel': ['linear', 'rbf'], \n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__epsilon': [0.1, 0.2, 0.5]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f853173",
   "metadata": {},
   "source": [
    "**Note : gradient boosting and support vector machine are taking a long time to execute, and have therefore not been used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d795c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to include in the pipeline\n",
    "selected_models = ['linear_regression', 'ridge_regression', 'lasso_regression', \n",
    "                   'polynomial_regression', 'random_forest', 'decision_tree', 'xgboost']\n",
    "# 'gradient_boosting', 'support_vector_regression'\n",
    "\n",
    "# Filter the models dictionary to only include the selected models\n",
    "filtered_models = {model_name: models[model_name] for model_name in selected_models}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd19fc1",
   "metadata": {},
   "source": [
    "## B. Hyperparameter Tuning and Pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95246b",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning** is important to find the optimal hyperparameters to make the model perform well on the training set. To reduce the chances of overfitting, the different hyperparameter values are performed on a K Fold cross validation set to provide a robust model. `Grid Search CV` allows providing traditionally accepted values for different hyperparameters and find an optimal combination from within them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7035ed3",
   "metadata": {},
   "source": [
    "**Pipelining**: To execute multiple combinations of hyperparameters for many models, it is important to set up clean model pipelines. This is done through sklearn's `pipeline` module. GridSearchCV along with pipeline utilizes CPU cores in parallel to execute the code faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc95ab4",
   "metadata": {},
   "source": [
    "**Loss Function**: It measures how well the model's predictions match the actual target values and guides the optimization process by providing a quantitative measure of the model's performance.\n",
    "\n",
    "There are many loss functions which can be used. However, it is best to choose a function that aligns with the objective of the task i.e. housing price prediction, along with understanding of the data and the final interpretability. From EDA, we know that there may be some outliers present in the data, even after applying log transformations.\n",
    "\n",
    "Mean Absolute Error (MAE) treats all errors equally, regardless of magnitude; it is robust to outliers and provides a better interpretable measure of error compared to Mean Squared Error (MSE) - as it is in the same unit as the airbnb price. MSE is more appropriate if the model is required to penalize larger errors heavily; or that there are no outliers (since MSE is sensitive to outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e81b7",
   "metadata": {},
   "source": [
    "Note : `GridSearchCV` aims to maximize the scoring metrics. Therefore, to minimize the MAE, we should try to maximize the negative MAE, which is done in the scoring parameter of Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a1a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "#Initialize the GridSearchCV results dictionary\n",
    "grid_search_results = {}\n",
    "\n",
    "# Create and evaluate pipelines for each model\n",
    "for model_name, model in filtered_models.items():\n",
    "    start_time = time.time() \n",
    "    pipeline = Pipeline([\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline, # pipeline with the model.\n",
    "        param_grid=param_grid[model_name], # parameter grid for the current model\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='neg_mean_absolute_error',  # Using negative mean absolute error as scoring\n",
    "        n_jobs=-1  # Uses all available CPU cores for parallel processing.\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Store the results\n",
    "    grid_search_results[model_name] = grid_search\n",
    "    end_time = time.time()  # End timing\n",
    "    duration = end_time - start_time  # Calculate duration\n",
    "    print(f\"Time taken to fit {model_name}: {duration} seconds\")\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Best Parameters': grid_search.best_params_,\n",
    "        'Best CV Score': grid_search.best_score_,\n",
    "        'Mean Absolute Error': mean_absolute_error(y_test, grid_search.best_estimator_.predict(X_test)),\n",
    "        'R^2 Score': r2_score(y_test, grid_search.best_estimator_.predict(X_test)),\n",
    "        'Time Taken (seconds)': duration\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f52cb",
   "metadata": {},
   "source": [
    "# 3. Evaluating Models on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644e0bf",
   "metadata": {},
   "source": [
    "For each model, `grid_search` is executed, which stores the following information: \n",
    "\n",
    "- `grid_search.best_params_`: a dictionary containing the hyperparameters that resulted in the best performance according to the specified scoring metric i.e. MAE.\n",
    "\n",
    "- `grid_search.best_score_ `: This attribute gives the best score achieved by the model using the best hyperparameters found during the grid search.\n",
    "\n",
    "- `grid_search.best_estimator_`: provides the actual model (estimator) instance that was trained with the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e09ef93",
   "metadata": {},
   "source": [
    "The performance of the models are measured using the test dataset. The `R2 Score` metric is also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ccacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of the best performances of each model on training set.\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1806ccb",
   "metadata": {},
   "source": [
    "Best Model will be the one with the lowest Mean Absolute Error. R2 scores can be used for comparison as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b177b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "lowest_mae = float('inf')\n",
    "\n",
    "for result in results:\n",
    "    if result['Mean Absolute Error'] < lowest_mae:\n",
    "        lowest_mae = result['Mean Absolute Error']\n",
    "        best_model = result\n",
    "\n",
    "# Print the best model details\n",
    "print(\"Best Model based on MAE:\")\n",
    "print(f\"Model: {best_model['Model']}\")\n",
    "print(f\"Best Parameters: {best_model['Best Parameters']}\")\n",
    "print(f\"Best CV Score: {best_model['Best CV Score']}\")\n",
    "print(f\"Mean Absolute Error: {best_model['Mean Absolute Error']}\")\n",
    "print(f\"R^2 Score: {best_model['R^2 Score']}\")\n",
    "print(f\"Time Taken (seconds): {best_model['Time Taken (seconds)']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c1899",
   "metadata": {},
   "source": [
    "# 4. Retraining the best model again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1718c8e",
   "metadata": {},
   "source": [
    "Once the best model with its optimal hyperparameters using cross-validation on a training set are identified, it's beneficial (and a best practice) to train the model on all available training data to potentially improve its performance - thereby improving generalization. This is usually possible, when the retraining is not computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e025330",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = best_model['Model']\n",
    "\n",
    "# Retrieve the best pipeline for the best model\n",
    "best_model_pipeline = grid_search_results[best_model_name].best_estimator_\n",
    "\n",
    "# Train the best pipeline on the entire training dataset\n",
    "best_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Test the trained best pipeline on the test dataset\n",
    "test_predictions = best_model_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the best pipeline on the test dataset\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2_score = r2_score(y_test, test_predictions)\n",
    "\n",
    "print(\"Best Model:\", best_model_name)\n",
    "print(\"Best Parameters:\", best_model_pipeline.named_steps['model'].get_params())  # Parameters of the model in the best pipeline\n",
    "print(\"Mean Absolute Error on Test Set:\", test_mae)\n",
    "print(\"R^2 Score on Test Set:\", test_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883a9da",
   "metadata": {},
   "source": [
    "There is not much improvement in the result, but can sometimes lead to better generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ef505",
   "metadata": {},
   "source": [
    "# 5. Interpretation of Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870d998",
   "metadata": {},
   "source": [
    "## Actual vs Predicted Values\n",
    "\n",
    "- The x-axis represents the actual values (ground truth) from the test set (y_test), and the y-axis represents the predicted values (test_predictions).\n",
    "\n",
    "- Each point in the scatter plot represents a data point from the test set, where the x-coordinate is the actual value, and the y-coordinate is the predicted value.\n",
    "\n",
    "- The dashed line represents the line where actual values equal predicted values (perfect prediction). Points close to this line indicate accurate predictions.\n",
    "\n",
    "- The blue dots represent the actual vs. predicted values. We plot the perfect prediction line for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51feeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a scatter plot to compare the actual vs. predicted values for each model.\n",
    "# Plot predicted vs. actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "# for name, model in models.items():\n",
    "#     y_pred = model.predict(X_test)\n",
    "plt.scatter(y_test, test_predictions, label=best_model_name)\n",
    "\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98f313",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "It is a post-training analysis technique used to understand which features (variables) have the most significant impact on the model's predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_model_pipeline.named_steps['model']\n",
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ae745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for feature importances or coefficients\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_importances = final_model.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "    feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "    sorted_feature_importance = dict(sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    print(\"Feature Importance (from feature_importances_):\")\n",
    "    for feature, importance in sorted_feature_importance.items():\n",
    "        print(f\"{feature}: {importance}\")\n",
    "    \n",
    "    # Plotting the feature importances\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances, align='center')\n",
    "    plt.xticks(range(len(feature_importances)), feature_names, rotation=90)\n",
    "    plt.title('Feature Importances')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "\n",
    "elif hasattr(final_model, 'coef_'):\n",
    "    # For linear models\n",
    "    feature_importances = final_model.coef_\n",
    "    feature_names = X_train.columns\n",
    "    feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "    sorted_feature_importance = dict(sorted(feature_importance_dict.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "    print(\"Feature Importance (from coef_):\")\n",
    "    for feature, importance in sorted_feature_importance.items():\n",
    "        print(f\"{feature}: {importance}\")\n",
    "    \n",
    "    # Plotting the feature importances\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances, align='center')\n",
    "    plt.xticks(range(len(feature_importances)), feature_names, rotation=90)\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Feature importance not available for this model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e718928d",
   "metadata": {},
   "source": [
    "## Residual Analysis\n",
    "\n",
    "It helps to understand how well the model fits the data by examining the differences between the observed values and the predicted values (residuals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b0357",
   "metadata": {},
   "source": [
    "Residual plot is a scatter plot which helps to check if residuals have any specific pattern. Ideally, it should be random (which it is, in the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_predictions, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f0622",
   "metadata": {},
   "source": [
    "Residual Distribution should ideally be approximately normally distributed. This is because the errors should be random (no pattern), and if so, they should obey the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de486640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals vs. each predictor variable\n",
    "for col in X_test.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_test[col], residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title(f'Residuals vs. {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611e97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d468ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c0a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a24ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
